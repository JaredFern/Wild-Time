{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from synthetic import shape_means, shape_sigmas\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_at_alpha(source_means, source_sigmas, target_means, target_sigmas, alpha):\n",
    "    num_classes = source_means.shape[0]\n",
    "    class_prob = 1.0 / num_classes\n",
    "    y = np.argmax(np.random.multinomial(1, [class_prob] * num_classes))\n",
    "    mean = source_means[y] * (1 - alpha) + target_means[y] * alpha\n",
    "    sigma = source_sigmas[y] * (1 - alpha) + target_sigmas[y] * alpha\n",
    "    x = np.random.multivariate_normal(mean, sigma)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "N_INTER = 8\n",
    "N_train = 128\n",
    "N_val = 64\n",
    "means, var_list = [], []\n",
    "for i in range(4):\n",
    "    means.append(np.random.multivariate_normal(np.random.randint(2, size=d), np.eye(d)))\n",
    "    means[i] = means[i] / np.linalg.norm(means[i])\n",
    "    # Generate diagonal.\n",
    "    diag = np.diag(np.random.uniform(0.1, 0.3, size=d))\n",
    "    rot = ortho_group.rvs(d)\n",
    "    var = np.matmul(rot, np.matmul(diag, np.linalg.inv(rot)))\n",
    "    var_list.append(var)\n",
    "\n",
    "for i in range(1):\n",
    "    means[3][i] = means[0][i]\n",
    "\n",
    "source_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), 0) for _ in range(N_train)]\n",
    "source_x_train = np.stack([x for x, y in source_pairs])\n",
    "source_y_train = np.stack([y for x, y in source_pairs])\n",
    "source_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), 0) for _ in range(N_val)]\n",
    "source_x_val = np.stack([x for x, y in source_pairs])\n",
    "source_y_val = np.stack([y for x, y in source_pairs])\n",
    "\n",
    "target_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), 1) for _ in range(N_train)]\n",
    "target_x_train = np.stack([x for x, y in target_pairs])\n",
    "target_y_train = np.stack([y for x, y in target_pairs])\n",
    "target_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), 1) for _ in range(N_val)]\n",
    "target_x_val = np.stack([x for x, y in target_pairs])\n",
    "target_y_val = np.stack([y for x, y in target_pairs])\n",
    "\n",
    "alphas = np.linspace(0, 1, N_INTER)\n",
    "train_x = [source_x_train]\n",
    "train_y = [source_y_train]\n",
    "val_x = [source_x_val]\n",
    "val_y = [source_y_val]\n",
    "for i, alpha in enumerate(alphas):\n",
    "    inter_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), alpha) for _ in range(N_train)]\n",
    "    inter_x = np.stack([x for x, y in inter_pairs])\n",
    "    inter_y = np.stack([y for x, y in inter_pairs])\n",
    "    train_x.append(inter_x)\n",
    "    train_y.append(inter_y)\n",
    "\n",
    "    inter_pairs = [get_gaussian_at_alpha(shape_means(means[:2]), shape_sigmas(var_list[2:], means[:2]), shape_means(means[2:]), shape_sigmas(var_list[2:], means[2:]), alpha) for _ in range(N_val)]\n",
    "    inter_x = np.stack([x for x, y in inter_pairs])\n",
    "    inter_y = np.stack([y for x, y in inter_pairs])\n",
    "    val_x.append(inter_x)\n",
    "    val_y.append(inter_y)\n",
    "train_x.append(target_x_train)\n",
    "train_y.append(target_y_train)\n",
    "val_x.append(target_x_val)\n",
    "val_y.append(target_y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, type, train_x, train_y, h_size=None):\n",
    "        if type == \"linear\":\n",
    "            self.model = LogisticRegression(random_state=0).fit(train_x, train_y)\n",
    "        elif type == \"mlp\":\n",
    "            self.model = MLPClassifier(hidden_layer_sizes=h_size, random_state=0, batch_size=8, max_iter=256).fit(train_x, train_y)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        return self.model.score(x, y)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.model.fit(x, y)\n",
    "    \n",
    "    def flatten(self):\n",
    "        if isinstance(self.model, LogisticRegression):\n",
    "            return deepcopy(np.hstack([self.model.coef_.flatten(), self.model.intercept_.flatten()]))\n",
    "        elif isinstance(self.model, MLPClassifier):\n",
    "            return deepcopy(\n",
    "                np.hstack([\n",
    "                    np.hstack([p.flatten() for p in self.model.coefs_]), \n",
    "                    np.hstack([p.flatten() for p in self.model.intercepts_])\n",
    "                    ]))\n",
    "    \n",
    "    def set_weights(self, w):\n",
    "        if isinstance(self.model, LogisticRegression):\n",
    "            self.model.coef_ = w[:self.model.coef_.size].reshape(self.model.coef_.shape)\n",
    "            self.model.intercept_ = w[self.model.coef_.size:].reshape(self.model.intercept_.shape)\n",
    "        elif isinstance(self.model, MLPClassifier):\n",
    "            start = 0\n",
    "            for i, p in enumerate(self.model.coefs_):\n",
    "                end = start + p.size\n",
    "                self.model.coefs_[i] = w[start:end].reshape(self.model.coefs_[i].shape)\n",
    "                start = end\n",
    "            \n",
    "            for i, p in enumerate(self.model.intercepts_):\n",
    "                end = start + p.size\n",
    "                self.model.intercepts_[i] = w[start:end].reshape(self.model.intercepts_[i].shape)\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "ewa_weights = []\n",
    "for i, (x_tr, y_tr, x_v, y_v) in enumerate(zip(train_x[:-1], train_y[:-1], val_x[:-1], val_y[:-1])):\n",
    "    if i == 0:\n",
    "        clf = Model(\"linear\", x_tr, y_tr)\n",
    "    else:\n",
    "        clf = Model(\"linear\", x_tr, y_tr)\n",
    "        clf.set_weights(weights[-1])\n",
    "        clf.fit(x_tr, y_tr)\n",
    "    \n",
    "    ood_scores = np.array([clf.score(x, y) for x, y in zip(val_x[i + 1:], val_y[i + 1:])])\n",
    "    print(f\"FT   | TIME {i}: ID = {clf.score(x_v, y_v)}; OOD mean = {ood_scores.mean()}; OOD worst = {ood_scores.min()}; OOD next = {clf.score(val_x[i + 1], val_y[i + 1])}\")\n",
    "\n",
    "    if i == 0:\n",
    "        ewa_weights.append(clf.flatten())\n",
    "    else:\n",
    "        ewa_weights.append(0.5 * ewa_weights[-1] + 0.5 * clf.flatten())\n",
    "    \n",
    "    ewa_clf = deepcopy(clf)\n",
    "    ewa_clf.set_weights(ewa_weights[-1])\n",
    "    ood_scores = np.array([ewa_clf.score(x, y) for x, y in zip(val_x[i + 1:], val_y[i + 1:])])\n",
    "    print(f\"CSAW | TIME {i}: ID = {ewa_clf.score(x_v, y_v)}; OOD mean = {ood_scores.mean()}; OOD worst = {ood_scores.min()}; OOD next = {ewa_clf.score(val_x[i + 1], val_y[i + 1])}\")\n",
    "    print()\n",
    "    weights.append(clf.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the classification boundary using a line\n",
    "def make_plot(x, y, ws, bs, styles, cmap='bwr'):\n",
    "    for w, b, style in zip(ws, bs, styles):\n",
    "        a = -w[0] / w[1]\n",
    "        xx = np.linspace(x[:, 0].min() - 1, x[:, 0].max() + 1)\n",
    "        yy = a * xx - b / w[1]\n",
    "        plt.plot(xx, yy, style)\n",
    "\n",
    "    # Plot the data points with different colors indicating the labels\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap, alpha=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(np.stack(train_x).reshape((12 * 200, -1)))\n",
    "train_x_embedded = [X_embedded[i * 200:(i + 1) * 200] for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"synthetic_1.npy\", np.stack({\n",
    "    \"weights\": weights,\n",
    "    \"intercepts\": intercepts,\n",
    "    \"ewa_weights\": ewa_weights,\n",
    "    \"ewa_intercepts\": ewa_intercepts,\n",
    "    \"train_x\": train_x,\n",
    "    \"train_y\": train_y,\n",
    "    \"val_x\": val_x,\n",
    "    \"val_y\": val_y,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_contours(\n",
    "    w1, w2, w3, val_x, val_y, granularity=20, margin=0.2, model_ids=None, method='erm'\n",
    "):\n",
    "    \"\"\"Runs the loss contour analysis.\n",
    "    Creates plane based on the parameters of 3 models, and computes loss and accuracy\n",
    "    contours on that plane. Specifically, computes 2 axes based on the 3 models, and\n",
    "    computes metrics on points defined by those axes.\n",
    "    Args:\n",
    "        model1: Origin of plane.\n",
    "        model2: Model used to define y axis of plane.\n",
    "        model3: Model used to define x axis of plane.\n",
    "        dataloader: Dataloader for the dataset to evaluate on.\n",
    "        eval_fn: A function that takes a model, a dataloader, and a device, and returns\n",
    "            a dictionary with two metrics: \"loss\" and \"accuracy\".\n",
    "        device: Device that the model and data should be moved to for evaluation.\n",
    "        granularity: How many segments to divide each axis into. The model will be\n",
    "            evaluated at granularity*granularity points.\n",
    "        margin: How much margin around models to create evaluation plane.\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    # Define x axis\n",
    "    u = w3 - w1\n",
    "    dx = np.norm(u).item()\n",
    "    u /= dx\n",
    "\n",
    "    # Define y axis\n",
    "    v = w2 - w1\n",
    "    v -= np.dot(u, v) * u\n",
    "    dy = np.norm(v).item()\n",
    "    v /= dy\n",
    "\n",
    "    # Define grid representing parameters that will be evaluated.\n",
    "    coords = np.stack(get_xy(p, w1, u, v) for p in [w1, w2, w3])\n",
    "    alphas = np.linspace(0.0 - margin, 1.0 + margin, granularity)\n",
    "    betas = np.linspace(0.0 - margin, 1.0 + margin, granularity)\n",
    "    losses = np.zeros((granularity, granularity))\n",
    "    accuracies = np.zeros((granularity, granularity))\n",
    "    grid = np.zeros((granularity, granularity, 2))\n",
    "\n",
    "    # Evaluate parameters at every point on grid\n",
    "    # progress = tqdm(total=granularity * granularity)\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        for j, beta in enumerate(betas):\n",
    "            p = w1 + alpha * dx * u + beta * dy * v\n",
    "            y_pred = clf.predict_proba(val_x)[:, 1]\n",
    "            grid[i, j] = [alpha * dx, beta * dy]\n",
    "            losses[i, j] = loss = log_loss(y_val, y_pred)\n",
    "            accuracies[i, j] = metrics[\"accuracy\"]\n",
    "            progress.update()\n",
    "    progress.close()\n",
    "    return {\n",
    "        \"grid\": grid,\n",
    "        \"coords\": coords,\n",
    "        \"losses\": losses,\n",
    "        \"accuracies\": accuracies,\n",
    "        \"model_ids\": model_ids\n",
    "    }\n",
    "\n",
    "def get_xy(point, origin, vector_x, vector_y):\n",
    "    \"\"\"Return transformed coordinates of a point given parameters defining coordinate\n",
    "    system.\n",
    "    Args:\n",
    "        point: point for which we are calculating coordinates.\n",
    "        origin: origin of new coordinate system\n",
    "        vector_x: x axis of new coordinate system\n",
    "        vector_y: y axis of new coordinate system\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            np.dot(point - origin, vector_x).item(),\n",
    "            np.dot(point - origin, vector_y).item(),\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildtime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
